{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "urls = [\n",
    "    \"https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/\",\n",
    "    \"https://link.springer.com/article/10.1007/s11831-021-09694-4\"\n",
    "    \"https://en.wikipedia.org/wiki/Particle_swarm_optimization#:~:text=In%20computational%20science%2C%20particle%20swarm,a%20given%20measure%20of%20quality.\",\n",
    "]\n",
    "\n",
    "local_llm = \"deepseek-r1:8b\"\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma-pso-1\",\n",
    "    embedding=OllamaEmbeddings(model=local_llm),\n",
    "\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arneau/anaconda3/envs/llm-env/lib/python3.10/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what \"agent memory\" refers to based on the given context. Let me look through the documents provided.\n",
      "\n",
      "The first document has metadata with a title about an error related to an ambiguous URI empty segment. The page content mentions an HTTP 400 error and some technical details about URIs. It doesn't seem directly related to agent memory or AI concepts.\n",
      "\n",
      "The second document's metadata includes a source link from machinelearningmastery.com, which is relevant because it's about particle swarm optimization. The title says \"Just a moment...\" but the page content is just placeholders, so I can't get much info there either.\n",
      "\n",
      "Putting this together, both documents don't directly discuss agent memory or AI agents. They focus on technical errors and particle swarm optimization. So, I don't have enough information to answer what agent memory specifically refers to in these contexts.\n",
      "</think>\n",
      "\n",
      "Agent memory isn't explicitly discussed in the provided context. The documents mention an HTTP error and particle swarm optimization but don't elaborate on agent memory.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents}\n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 'no'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n\n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation}\n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'<think>\\nOkay, so I need to figure out how to improve the given question about \"agent memory\" for better vector store retrieval. Let me start by understanding what the user is asking for.\\n\\nThe original question is just \"agent memory.\" That\\'s pretty vague. I guess the goal here is to make it more specific so that when someone uses a vector store or search engine, they can get more relevant results. Vector stores often rely on embeddings or vectors that represent data in a way that allows for efficient similarity searches.\\n\\nSo, what does \"agent memory\" refer to? It could be related to artificial intelligence agents remembering past interactions, data storage, or something else. Without more context, it\\'s hard to pin down exactly. The user wants the question improved so that it\\'s optimized for vector store retrieval. That probably means making it a precise query that can match well with stored vectors.\\n\\nI should think about how to rephrase \"agent memory\" into a query that\\'s more specific. Maybe breaking it down into components: what aspects of agent memory are we talking about? Is it data storage, decision-making processes, or something else?\\n\\nPerhaps the improved question should include terms like \"data structure,\" \"storage mechanism,\" or \"memory optimization.\" Also, specifying the context could help, such as in machine learning models or AI systems.\\n\\nWait, maybe I can make it more action-oriented. Instead of just asking about memory, ask how agent memory is structured or stored. That way, the query becomes a question about the mechanisms behind it, which might align better with vector store retrievals that look for specific types of information.\\n\\nSo, putting it all together, an improved version could be something like: \"What are the data structures or storage mechanisms used in agent memory systems?\" Or maybe more focused on retrieval: \"How is agent memory stored and retrieved within AI systems?\"\\n\\nI think including terms like \"data structure,\" \"storage mechanism,\" and specifying the context (like AI systems) would make the query more precise, which should help with better vector store retrieval results.\\n</think>\\n\\n**Improved Question:**  \\n\"What are the data structures or storage mechanisms used in agent memory systems?\"'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('<think>\\n'\n",
      " 'Okay, so I need to explain how the Particle Swarm Optimization (PSO) '\n",
      " \"algorithm works. From what I remember, it's a type of optimization algorithm \"\n",
      " 'used in computational science for finding the best solution to a problem. '\n",
      " 'The main idea is inspired by the behavior of particles moving around in a '\n",
      " 'swarm, like birds or fish.\\n'\n",
      " '\\n'\n",
      " 'So, there are particles in this system. Each particle has a position and a '\n",
      " 'velocity. They move around in a search space, which is the area where the '\n",
      " \"solutions to the problem might be found. The particles' movements are guided \"\n",
      " \"by some rules based on their neighbors' positions. I think each particle \"\n",
      " \"also keeps track of the best solution it's encountered so far.\\n\"\n",
      " '\\n'\n",
      " 'The algorithm starts with a population of particles, and they all begin '\n",
      " 'moving in random directions. As they move, they evaluate how good their '\n",
      " 'current position is compared to the best solution found so far. If a '\n",
      " 'particle finds a better solution, it updates its own best position.\\n'\n",
      " '\\n'\n",
      " 'There\\'s also this concept called the \"fitness function,\" which measures how '\n",
      " 'good a solution is. The particles use this function to decide if their '\n",
      " 'current position is better than their previous best. If it is, they update '\n",
      " 'their best position.\\n'\n",
      " '\\n'\n",
      " 'Another important part is the mutation or variation step. This happens when '\n",
      " 'a particle encounters another particle that has a better solution. It then '\n",
      " 'has a chance to mutate its own position slightly, which might lead it to a '\n",
      " 'new area of the search space. This helps in exploring different regions and '\n",
      " 'not getting stuck in one spot.\\n'\n",
      " '\\n'\n",
      " 'The particles also communicate with each other. They compare their positions '\n",
      " 'and swap them if necessary, which helps in moving towards areas that are '\n",
      " 'more promising for better solutions. Over time, the particles should '\n",
      " 'converge towards the region where the best solution is located.\\n'\n",
      " '\\n'\n",
      " 'I think the algorithm runs for a certain number of iterations, and after '\n",
      " \"each iteration, it checks if the best solution has improved. If it hasn't \"\n",
      " \"improved for a set number of iterations, the algorithm stops, assuming it's \"\n",
      " 'found the optimal solution or at least a good approximation.\\n'\n",
      " '\\n'\n",
      " \"Wait, but I'm not entirely sure about all these steps. Maybe I should break \"\n",
      " 'it down more clearly to make sure I understand each part correctly.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'Particle Swarm Optimization (PSO) is an optimization algorithm inspired by '\n",
      " \"the social behavior of particles in a swarm. Here's how it works:\\n\"\n",
      " '\\n'\n",
      " '1. **Initialization**: A population of particles is initialized, each with a '\n",
      " 'position and velocity. These particles search within a defined space.\\n'\n",
      " '\\n'\n",
      " '2. **Fitness Evaluation**: Each particle evaluates its current position '\n",
      " 'using a fitness function, which measures the quality of the solution '\n",
      " 'relative to the problem being optimized.\\n'\n",
      " '\\n'\n",
      " '3. **Particle Movement**: Particles move based on their velocities and the '\n",
      " \"influence of neighboring particles' positions. They update their best-known \"\n",
      " 'solution if they find a better position.\\n'\n",
      " '\\n'\n",
      " '4. **Mutation and Communication**: When a particle encounters a better '\n",
      " 'solution, it may mutate its position slightly. Particles also communicate by '\n",
      " 'comparing positions, swapping to explore more promising areas.\\n'\n",
      " '\\n'\n",
      " '5. **Convergence**: Over iterations, particles converge towards regions with '\n",
      " 'better solutions. The algorithm stops when no improvement is detected for a '\n",
      " 'set number of iterations, typically indicating the best solution found.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain in detail how a Particle Swarm Optimization algorithm works?\"}\n",
    "for output in app.stream(inputs, {\"recursion_limit\": 5}):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "# pprint(value[\"generation\"])\n",
    "pprint(value[\"generation\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('<think>\\n'\n",
      " 'Okay, so I need to explain Particle Swarm Optimization (PSO). From the '\n",
      " 'context provided, it seems like PSO is a computational method used for '\n",
      " \"optimization problems. It's inspired by the movement of particles in a \"\n",
      " 'swarm, like birds or fish, searching for food or avoiding predators.\\n'\n",
      " '\\n'\n",
      " 'I remember that in PSO, each particle moves through a search space looking '\n",
      " 'for the best solution. The particles adjust their positions based on their '\n",
      " 'own experience and the experiences of others in the swarm. This is similar '\n",
      " 'to how social behaviors work in nature.\\n'\n",
      " '\\n'\n",
      " 'The context mentions that PSO is used to find an optimal solution by '\n",
      " 'comparing the quality of solutions. So, each particle calculates its own '\n",
      " \"fitness, which measures how good a solution it's found is compared to \"\n",
      " 'others. The particles then move towards areas where better solutions have '\n",
      " 'been found.\\n'\n",
      " '\\n'\n",
      " 'I think PSO is part of computational science and is used in various '\n",
      " 'optimization problems like engineering design or machine learning tasks. '\n",
      " \"It's known for being a global search algorithm because the particles can \"\n",
      " 'explore different regions of the search space, which helps in finding robust '\n",
      " 'solutions.\\n'\n",
      " '\\n'\n",
      " \"However, I'm not entirely sure about all the specifics, like how exactly the \"\n",
      " 'particles update their positions or the exact parameters involved. But based '\n",
      " 'on what I know and the context provided, PSO is a nature-inspired '\n",
      " 'optimization technique that uses a swarm of particles to find the best '\n",
      " 'solution by comparing fitness values.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'Particle Swarm Optimization (PSO) is a computational method inspired by '\n",
      " 'social behaviors observed in nature, such as bird flocking or fish '\n",
      " 'schooling. It involves a swarm of particles moving through a search space to '\n",
      " 'find an optimal solution. Each particle adjusts its position based on its '\n",
      " 'own experience and the experiences of others in the swarm, using fitness '\n",
      " 'values to measure solution quality. PSO is used for various optimization '\n",
      " 'problems, known for its ability to perform global searches and find robust '\n",
      " 'solutions.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain Particle Swarm Optimisation\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "# pprint(value[\"generation\"])\n",
    "pprint(value[\"generation\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
